import streamlit as st
import os
from dotenv import load_dotenv

# --- ‡¶á‡¶Æ‡ßç‡¶™‡ßã‡¶∞‡ßç‡¶ü ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

load_dotenv()

st.set_page_config(page_title="PSNS: Study Notes", page_icon="üìö")
st.title("üìö Personal Study Notes Searcher")

openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("‚ö†Ô∏è API Key ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡ßü‡¶®‡¶ø!")
    st.stop()

if "vector_store" not in st.session_state:
    st.session_state.vector_store = None

# --- ‡ßß. ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶∏‡ßá‡¶ï‡¶∂‡¶® (‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã‡¶á) ---
uploaded_file = st.file_uploader("‡¶≤‡ßá‡¶ï‡¶ö‡¶æ‡¶∞ ‡¶∏‡ßç‡¶≤‡¶æ‡¶á‡¶° (PDF) ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßã", type=['pdf'])

if uploaded_file:
    if not os.path.exists("temp_files"):
        os.makedirs("temp_files")
    
    file_path = os.path.join("temp_files", uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    if st.button("üß† ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®"):
        with st.spinner("‡¶¨‡ßç‡¶∞‡ßá‡¶á‡¶® ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡¶ö‡ßç‡¶õ‡ßá..."):
            try:
                loader = PyPDFLoader(file_path)
                pages = loader.load()

                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                chunks = text_splitter.split_documents(pages)
                
                embeddings = OpenAIEmbeddings()
                vector_store = FAISS.from_documents(chunks, embeddings)
                
                st.session_state.vector_store = vector_store
                st.success(f"‚úÖ ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶®! {len(pages)} ‡¶™‡ßá‡¶ú ‡¶™‡ßú‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§")

            except Exception as e:
                st.error(f"Error: {e}")

st.write("---")
# --- ‡ß®. ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ì ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶∏‡ßá‡¶ï‡¶∂‡¶® (Fixed Version) ---
user_question = st.text_input("‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡ßã:")

if user_question and st.session_state.vector_store:
    
    # A. ‡¶∞‡¶ø‡¶ü‡ßç‡¶∞‡¶ø‡¶≠‡¶æ‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø
    retriever = st.session_state.vector_store.as_retriever()
    
    # B. ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ü‡ßá‡¶Æ‡¶™‡ßç‡¶≤‡ßá‡¶ü
    template = """Answer the question based ONLY on the following context:
    {context}
    
    Question: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

    with st.spinner("‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶õ‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶õ‡¶ø..."):
        try:
            # ‡¶ß‡¶æ‡¶™ ‡ßß: ‡¶Ü‡¶ó‡ßá ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡¶ó‡ßÅ‡¶≤‡ßã (Chunks) ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶ø
            relevant_docs = retriever.invoke(user_question)
            
            # ‡¶ß‡¶æ‡¶™ ‡ß®: ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá ‡¶ï‡¶®‡¶≠‡¶æ‡¶∞‡ßç‡¶ü ‡¶ï‡¶∞‡¶ø (Manual Formatting)
            # ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø Python List Comprehension ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶õ‡¶ø, ‡¶Ø‡¶æ ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∏‡ßá‡¶´
            context_text = "\n\n".join([d.page_content for d in relevant_docs])
            
            # ‡¶ß‡¶æ‡¶™ ‡ß©: AI-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡ßç‡¶∞‡¶Æ‡ßç‡¶™‡¶ü ‡¶∞‡ßá‡¶°‡¶ø ‡¶ï‡¶∞‡¶æ
            formatted_prompt = prompt.invoke({"context": context_text, "question": user_question})
            
            # ‡¶ß‡¶æ‡¶™ ‡ß™: ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ
            response = llm.invoke(formatted_prompt)
            
            # ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã
            st.success("‡¶â‡¶§‡ßç‡¶§‡¶∞:")
            st.write(response.content) # .content ‡¶¶‡¶ø‡¶≤‡ßá ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶Ü‡¶∏‡¶¨‡ßá
            
            # ‡¶ß‡¶æ‡¶™ ‡ß´: ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã (Page Numbers)
            st.warning("üìå ‡¶∞‡ßá‡¶´‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶∏ (Sources):")
            
            unique_pages = set()
            for doc in relevant_docs:
                # ‡¶™‡ßá‡¶ú ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ (‡¶Ø‡¶¶‡¶ø ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá ‡¶§‡¶¨‡ßá 0 ‡¶ß‡¶∞‡¶¨‡ßá)
                page_num = doc.metadata.get('page', 0) + 1
                unique_pages.add(page_num)
            
            for page in sorted(unique_pages):
                st.write(f"üìÑ ‡¶§‡¶•‡ßç‡¶Ø‡¶ü‡¶ø **Page {page}** ‡¶•‡ßá‡¶ï‡ßá ‡¶®‡ßá‡¶ì‡ßü‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§")
                
            # ‡¶°‡¶ø‡¶¨‡¶æ‡¶ó‡¶ø‡¶Ç (‡¶Ö‡¶™‡¶∂‡¶®‡¶æ‡¶≤)
            with st.expander("üîç ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®"):
                for i, doc in enumerate(relevant_docs):
                    st.caption(f"Source {i+1} (Page {doc.metadata.get('page', 0) + 1})")
                    st.text(doc.page_content[:200] + "...")

        except Exception as e:
            st.error(f"Error: {e}")

elif user_question and not st.session_state.vector_store:
    st.warning("‚ö†Ô∏è ‡¶Ü‡¶ó‡ßá ‡¶´‡¶æ‡¶á‡¶≤ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡ßã!")