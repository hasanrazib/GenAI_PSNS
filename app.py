import streamlit as st
import os
from dotenv import load_dotenv

# --- ‡¶Ü‡¶ß‡ßÅ‡¶®‡¶ø‡¶ï ‡¶á‡¶Æ‡ßç‡¶™‡ßã‡¶∞‡ßç‡¶ü (LCEL - Modern Approach) ---
# ‡¶è‡¶á ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø‡¶ó‡ßÅ‡¶≤‡ßã ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶™‡¶ø‡¶∏‡¶ø‡¶§‡ßá ‡¶Ö‡¶≤‡¶∞‡ßá‡¶°‡¶ø ‡¶†‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá ‡¶á‡¶®‡ßç‡¶∏‡¶ü‡¶≤ ‡¶Ü‡¶õ‡ßá
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# .env ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ
load_dotenv()

st.set_page_config(page_title="PSNS: Study Notes Searcher", page_icon="üìö")
st.title("üìö Personal Study Notes Searcher")

# API Key ‡¶ö‡ßá‡¶ï
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("‚ö†Ô∏è .env ‡¶´‡¶æ‡¶á‡¶≤‡ßá API Key ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡ßü‡¶®‡¶ø!")
    st.stop()

# ‡¶Æ‡ßá‡¶Æ‡ßã‡¶∞‡¶ø ‡¶∏‡ßç‡¶ü‡ßá‡¶ü
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None

# --- ‡ßß. ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶∏‡ßá‡¶ï‡¶∂‡¶® ---
uploaded_file = st.file_uploader("‡¶≤‡ßá‡¶ï‡¶ö‡¶æ‡¶∞ ‡¶∏‡ßç‡¶≤‡¶æ‡¶á‡¶° (PDF) ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßã", type=['pdf'])

if uploaded_file:
    # ‡¶ü‡ßá‡¶Æ‡ßç‡¶™ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡¶æ
    if not os.path.exists("temp_files"):
        os.makedirs("temp_files")
    
    file_path = os.path.join("temp_files", uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶¨‡¶æ‡¶ü‡¶®
    if st.button("üß† ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®"):
        with st.spinner("‡¶¨‡ßç‡¶∞‡ßá‡¶á‡¶® ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡¶ö‡ßç‡¶õ‡ßá... (Smart LCEL Mode)"):
            try:
                # PDF ‡¶™‡ßú‡¶æ
                loader = PyPDFLoader(file_path)
                pages = loader.load()

                # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶ü‡ßÅ‡¶ï‡¶∞‡ßã ‡¶ï‡¶∞‡¶æ
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                chunks = text_splitter.split_documents(pages)
                
                # ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶°‡ßá‡¶ü‡¶æ‡¶¨‡ßá‡¶∏ ‡¶§‡ßà‡¶∞‡¶ø
                embeddings = OpenAIEmbeddings()
                vector_store = FAISS.from_documents(chunks, embeddings)
                
                # ‡¶∏‡ßá‡¶∂‡¶®‡ßá ‡¶∏‡ßá‡¶≠
                st.session_state.vector_store = vector_store
                st.success(f"‚úÖ ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶®! {len(pages)} ‡¶™‡ßá‡¶ú ‡¶™‡ßú‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§ ‡¶è‡¶ñ‡¶® ‡¶®‡¶ø‡¶ö‡ßá ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡ßã‡•§")

            except Exception as e:
                st.error(f"Error: {e}")

st.write("---")

# --- ‡ß®. ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ì ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶∏‡ßá‡¶ï‡¶∂‡¶® (LCEL ‡¶ö‡ßá‡¶á‡¶®) ---
user_question = st.text_input("‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡ßã:")

if user_question and st.session_state.vector_store:
    # A. ‡¶∞‡¶ø‡¶ü‡ßç‡¶∞‡¶ø‡¶≠‡¶æ‡¶∞ (Retriever) - ‡¶§‡¶•‡ßç‡¶Ø ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
    retriever = st.session_state.vector_store.as_retriever()
    
    # B. ‡¶™‡ßç‡¶∞‡¶Æ‡ßç‡¶™‡¶ü ‡¶ü‡ßá‡¶Æ‡¶™‡ßç‡¶≤‡ßá‡¶ü (AI ‡¶ï‡ßá ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂)
    template = """You are a helpful assistant for university students.
    Answer the question based ONLY on the following context:
    {context}
    
    Question: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    
    # C. LLM (GPT Model)
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
    
    # D. ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡¶ø‡¶Ç ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®
    def format_docs(docs):
        return "\n\n".join([d.page_content for d in docs])
    
    # E. ‡¶ö‡ßá‡¶á‡¶® ‡¶§‡ßà‡¶∞‡¶ø (LCEL ‡¶™‡¶æ‡¶á‡¶™‡¶≤‡¶æ‡¶á‡¶®)
    # Retriever -> Format -> Prompt -> LLM -> Output Parser
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    with st.spinner("‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶õ‡¶ø..."):
        try:
            response = rag_chain.invoke(user_question)
            st.success("‡¶â‡¶§‡ßç‡¶§‡¶∞:")
            st.write(response)
        except Exception as e:
            st.error(f"Error: {e}")

elif user_question and not st.session_state.vector_store:
    st.warning("‚ö†Ô∏è ‡¶Ü‡¶ó‡ßá ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßá '‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏' ‡¶¨‡¶æ‡¶ü‡¶®‡ßá ‡¶ö‡¶æ‡¶™ ‡¶¶‡¶æ‡¶ì!")